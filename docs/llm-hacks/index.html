<!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>LLM Hacks - Tech Blog</title>
        <meta name="description" content="Unlock LLM potential with expert prompt engineering hacks and techniques.">
        <meta name="keywords" content="LLM prompt engineering, software, natural language processing hacks, prompt engineering, PromptEngineering, AI model optimization., SustainableTech, prompt engineering techniques, AI prompt engineering, LLM optimization techniques, LLM hacks, language model optimization, AIAdvances, CleanCode, NaturalLanguage">
            <meta name="google-adsense-account" content="ca-pub-4477679588953789">
    <meta name="google-site-verification" content="AIzaSyBqIII5-K2quNev9w7iJoH5U4uqIqKDkEQ">
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-DST4PJYK6V"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-DST4PJYK6V');
    </script>
    <!-- Google AdSense -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4477679588953789" 
            crossorigin="anonymous"></script>

        
    <!-- SEO Meta Tags -->
    <meta name="description" content="Unlock LLM potential with expert prompt engineering hacks and techniques.">
    <meta property="og:title" content="LLM Hacks">
    <meta property="og:description" content="Unlock LLM potential with expert prompt engineering hacks and techniques.">
    <meta property="og:url" content="https://kubaik.github.io/llm-hacks/">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Tech Blog">
    <meta property="article:published_time" content="2026-02-06T22:33:31.062340">
    <meta property="article:modified_time" content="2026-02-06T22:33:31.062347">
    <meta property="og:image" content="/static/images/llm-hacks.jpg">
    <meta property="og:image:alt" content="LLM Hacks">
    <meta name="twitter:image" content="/static/images/llm-hacks.jpg">

    <!-- Twitter Cards -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="LLM Hacks">
    <meta name="twitter:description" content="Unlock LLM potential with expert prompt engineering hacks and techniques.">
    <meta name="twitter:site" content="@KubaiKevin">
    <meta name="twitter:creator" content="@KubaiKevin">

    <!-- Additional SEO -->
    <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1">
    <meta name="googlebot" content="index, follow">
    <link rel="canonical" href="https://kubaik.github.io/llm-hacks/">
    <meta name="keywords" content="LLM prompt engineering, software, natural language processing hacks, prompt engineering, PromptEngineering, AI model optimization., SustainableTech, prompt engineering techniques, AI prompt engineering, LLM optimization techniques, LLM hacks, language model optimization, AIAdvances, CleanCode, NaturalLanguage">
        <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLM Hacks",
  "description": "Unlock LLM potential with expert prompt engineering hacks and techniques.",
  "author": {
    "@type": "Organization",
    "name": "Tech Blog"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Tech Blog",
    "url": "https://kubaik.github.io",
    "logo": {
      "@type": "ImageObject",
      "url": "https://kubaik.github.io/static/logo.png"
    }
  },
  "datePublished": "2026-02-06T22:33:31.062340",
  "dateModified": "2026-02-06T22:33:31.062347",
  "url": "https://kubaik.github.io/llm-hacks/",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://kubaik.github.io/llm-hacks/"
  },
  "image": {
    "@type": "ImageObject",
    "url": "/static/images/llm-hacks.jpg"
  },
  "keywords": [
    "LLM prompt engineering",
    "software",
    "natural language processing hacks",
    "prompt engineering",
    "PromptEngineering",
    "AI model optimization.",
    "SustainableTech",
    "prompt engineering techniques",
    "AI prompt engineering",
    "LLM optimization techniques",
    "LLM hacks",
    "language model optimization",
    "AIAdvances",
    "CleanCode",
    "NaturalLanguage"
  ]
}
</script>
        <link rel="stylesheet" href="/static/style.css">
        <link rel="stylesheet" href="/static/enhanced-blog-post-styles.css">
    </head>
    <body>
        <!-- Header Ad Slot -->
        <div class="ad-header" style="text-align: center; margin: 20px 0;">
    <ins class="adsbygoogle"
         style="display:inline-block;width:728px;height:90px"
         data-ad-client="ca-pub-4477679588953789"
         
         data-ad-format="leaderboard"
         data-full-width-responsive="true"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
        
        <header>
            <div class="container">
                <h1><a href="/">Tech Blog</a></h1>
                <nav>
                    <a href="/">Home</a>
                    <a href="/about/">About</a>
                    <a href="/contact/">Contact</a>
                    <a href="/privacy-policy/">Privacy Policy</a>
                    <a href="/terms-of-service/">Terms of Service</a>
                </nav>
            </div>
        </header>
        <main class="container">
            <article class="blog-post">
                <header class="post-header">
                    <h1>LLM Hacks</h1>
                    <div class="post-meta">
                        <time datetime="2026-02-06T22:33:31.062340">2026-02-06</time>
                    </div>
                    
                    <div class="tags">
                        
                        <span class="tag">Cybersecurity</span>
                        
                        <span class="tag">LLM prompt engineering</span>
                        
                        <span class="tag">LLM hacks</span>
                        
                        <span class="tag">SustainableTech</span>
                        
                        <span class="tag">CleanCode</span>
                        
                        <span class="tag">software</span>
                        
                    </div>
                    
                </header>
                <div class="post-content">
                    <h2 id="introduction-to-prompt-engineering">Introduction to Prompt Engineering</h2>
<p>Prompt engineering is the process of designing and optimizing text prompts to elicit specific, accurate, and relevant responses from large language models (LLMs). As LLMs become increasingly powerful and ubiquitous, the need for effective prompt engineering has grown exponentially. In this article, we will delve into the world of prompt engineering, exploring its principles, techniques, and applications. We will also discuss common challenges and provide concrete solutions, along with code examples and real-world use cases.</p>
<h3 id="principles-of-prompt-engineering">Principles of Prompt Engineering</h3>
<p>Effective prompt engineering involves understanding the strengths and limitations of LLMs, as well as the specific task or application at hand. Some key principles of prompt engineering include:
* <strong>Specificity</strong>: Well-designed prompts should be specific, clear, and concise, avoiding ambiguity and vagueness.
* <strong>Contextualization</strong>: Providing context and background information can help LLMs better understand the prompt and generate more accurate responses.
* <strong>Granularity</strong>: Breaking down complex tasks into smaller, more manageable components can improve the accuracy and relevance of LLM responses.
* <strong>Iterative refinement</strong>: Prompt engineering is often an iterative process, requiring refinement and adjustment based on the results of initial attempts.</p>
<h2 id="practical-applications-of-prompt-engineering">Practical Applications of Prompt Engineering</h2>
<p>Prompt engineering has a wide range of practical applications, including but not limited to:
* <strong>Text classification</strong>: Using LLMs to classify text into specific categories, such as sentiment analysis or spam detection.
* <strong>Language translation</strong>: Leveraging LLMs to translate text from one language to another, with high accuracy and fluency.
* <strong>Content generation</strong>: Utilizing LLMs to generate high-quality content, such as articles, blog posts, or social media updates.
* <strong>Conversational interfaces</strong>: Building conversational interfaces, such as chatbots or voice assistants, that use LLMs to understand and respond to user input.</p>
<h3 id="code-example-text-classification-with-hugging-face-transformers">Code Example: Text Classification with Hugging Face Transformers</h3>
<p>The following code example demonstrates how to use the Hugging Face Transformers library to perform text classification using a pre-trained LLM:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="c1"># Load pre-trained LLM and tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>

<span class="c1"># Define dataset and labels</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;This is a positive review&quot;</span><span class="p">,</span> <span class="s2">&quot;This is a negative review&quot;</span><span class="p">],</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="p">})</span>

<span class="c1"># Preprocess text data</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Perform text classification</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Evaluate model performance</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p>This code example uses the Hugging Face Transformers library to load a pre-trained LLM and tokenizer, preprocess text data, perform text classification, and evaluate model performance.</p>
<h2 id="common-challenges-and-solutions">Common Challenges and Solutions</h2>
<p>Despite the many benefits of prompt engineering, there are several common challenges that practitioners may encounter, including:
* <strong>Overfitting</strong>: LLMs can suffer from overfitting, particularly when trained on small datasets or with limited contextual information.
* <strong>Underfitting</strong>: Conversely, LLMs may underfit if the prompt is too vague or lacking in context.
* <strong>Adversarial examples</strong>: LLMs can be vulnerable to adversarial examples, which are specifically designed to mislead or deceive the model.</p>
<p>To address these challenges, practitioners can use a range of techniques, including:
* <strong>Data augmentation</strong>: Increasing the size and diversity of the training dataset can help mitigate overfitting.
* <strong>Regularization techniques</strong>: Regularization techniques, such as dropout or L1/L2 regularization, can help prevent overfitting.
* <strong>Adversarial training</strong>: Training LLMs on adversarial examples can help improve their robustness and resilience.</p>
<h3 id="code-example-adversarial-training-with-pytorch">Code Example: Adversarial Training with PyTorch</h3>
<p>The following code example demonstrates how to use PyTorch to perform adversarial training on an LLM:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="c1"># Load pre-trained LLM and tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>

<span class="c1"># Define dataset and labels</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;This is a positive review&quot;</span><span class="p">,</span> <span class="s2">&quot;This is a negative review&quot;</span><span class="p">],</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="p">})</span>

<span class="c1"># Preprocess text data</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Define adversarial loss function</span>
<span class="k">def</span> <span class="nf">adversarial_loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="c1"># Perform adversarial training</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">][</span><span class="n">batch</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">][</span><span class="n">batch</span><span class="p">])</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">adversarial_loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">total_loss</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p>This code example uses PyTorch to perform adversarial training on an LLM, using a custom adversarial loss function and optimizing the model parameters using the Adam optimizer.</p>
<h2 id="real-world-use-cases">Real-World Use Cases</h2>
<p>Prompt engineering has a wide range of real-world use cases, including:
* <strong>Customer service chatbots</strong>: Building conversational interfaces that use LLMs to understand and respond to customer inquiries.
* <strong>Content generation platforms</strong>: Developing platforms that use LLMs to generate high-quality content, such as articles, blog posts, or social media updates.
* <strong>Language translation services</strong>: Leveraging LLMs to translate text from one language to another, with high accuracy and fluency.</p>
<h3 id="implementation-details">Implementation Details</h3>
<p>When implementing prompt engineering in real-world applications, there are several key considerations to keep in mind, including:
* <strong>Data quality</strong>: Ensuring that the training data is high-quality, diverse, and relevant to the specific task or application.
* <strong>Model selection</strong>: Choosing the most suitable LLM for the task or application, based on factors such as accuracy, fluency, and computational resources.
* <strong>Hyperparameter tuning</strong>: Adjusting hyperparameters, such as learning rate, batch size, and number of epochs, to optimize model performance.</p>
<h2 id="performance-benchmarks">Performance Benchmarks</h2>
<p>The performance of LLMs can vary significantly depending on the specific task, dataset, and model architecture. Some common performance benchmarks for LLMs include:
* <strong>Accuracy</strong>: Measuring the accuracy of LLM responses, either on a held-out test set or in a real-world deployment.
* <strong>Fluency</strong>: Evaluating the fluency and coherence of LLM-generated text, either using automated metrics or human evaluation.
* <strong>Computational resources</strong>: Assessing the computational resources required to train and deploy LLMs, including memory, CPU, and GPU usage.</p>
<h3 id="pricing-data">Pricing Data</h3>
<p>The cost of using LLMs can vary significantly depending on the specific platform, service, or model architecture. Some common pricing models for LLMs include:
* <strong>Per-token pricing</strong>: Charging users based on the number of tokens processed, either during training or deployment.
* <strong>Per-request pricing</strong>: Charging users based on the number of requests made to the LLM, either during training or deployment.
* <strong>Subscription-based pricing</strong>: Offering users a subscription-based model, with access to a fixed number of tokens or requests per month.</p>
<p>Some popular platforms and services for LLMs, along with their pricing data, include:
* <strong>Hugging Face Transformers</strong>: Offering a range of pre-trained LLMs, with pricing starting at $0.00006 per token.
* <strong>Google Cloud AI Platform</strong>: Providing a managed platform for LLM deployment, with pricing starting at $0.00045 per token.
* <strong>Amazon SageMaker</strong>: Offering a cloud-based platform for LLM deployment, with pricing starting at $0.00075 per token.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Prompt engineering is a critical component of large language model development, enabling practitioners to design and optimize text prompts that elicit specific, accurate, and relevant responses. By understanding the principles and techniques of prompt engineering, practitioners can build more effective and efficient LLMs, with applications in text classification, language translation, content generation, and conversational interfaces. To get started with prompt engineering, we recommend the following actionable next steps:
1. <strong>Explore pre-trained LLMs</strong>: Investigate popular pre-trained LLMs, such as BERT, RoBERTa, and DistilBERT, and experiment with different models and architectures.
2. <strong>Develop a prompt engineering workflow</strong>: Establish a workflow for designing, testing, and refining prompts, using techniques such as iterative refinement and adversarial training.
3. <strong>Evaluate model performance</strong>: Assess the performance of LLMs using metrics such as accuracy, fluency, and computational resources, and adjust hyperparameters and model architecture as needed.
4. <strong>Consider real-world applications</strong>: Explore real-world use cases for prompt engineering, such as customer service chatbots, content generation platforms, and language translation services.
5. <strong>Stay up-to-date with industry developments</strong>: Follow industry leaders, research papers, and conferences to stay informed about the latest advances and best practices in prompt engineering and LLM development.</p>
                    
                    <!-- Middle Ad Slot -->
                    <div class="ad-middle" style="text-align: center; margin: 20px 0;">
    <ins class="adsbygoogle"
         style="display:inline-block;width:300px;height:250px"
         data-ad-client="ca-pub-4477679588953789"
         
         data-ad-format="rectangle"
         data-full-width-responsive="true"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
                </div>
                
                <!-- Affiliate Disclaimer -->
                
            </article>
        </main>
        
        <!-- Footer Ad Slot -->
        <div class="ad-footer" style="text-align: center; margin: 20px 0;">
    <ins class="adsbygoogle"
         style="display:inline-block;width:468px;height:60px"
         data-ad-client="ca-pub-4477679588953789"
         
         data-ad-format="banner"
         data-full-width-responsive="true"></ins>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
        
        <footer>
            <div class="container">
                <p>&copy; 2026 Tech Blog.</p>
            </div>
        </footer>
        <!-- Enhanced Navigation Script -->
        <script src="/static/navigation.js"></script>
    </body>
    </html>